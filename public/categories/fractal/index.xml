<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>fractal on SHELTRON Visuals</title>
    <link>https://sheltron.netlify.com/categories/fractal/</link>
    <description>Recent content in fractal on SHELTRON Visuals</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 30 Dec 2017 00:13:37 -0706</lastBuildDate>
    
	<atom:link href="https://sheltron.netlify.com/categories/fractal/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>LEVELS</title>
      <link>https://sheltron.netlify.com/portfolio/2016-10-30-levels/</link>
      <pubDate>Sun, 30 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>https://sheltron.netlify.com/portfolio/2016-10-30-levels/</guid>
      <description>webgl audio-reactive iterative function system
Live Demo here play it some music with a beat and it should start dancing
while being a TA at Gray Area I worked on a fractal installation that I wanted to be educational.
Background  I had been working with Raymarched Distance Fields for about a year and found a few cool techniques I was happy with:
  raymarching at half-res for performance</description>
    </item>
    
    <item>
      <title>Synesthesia</title>
      <link>https://sheltron.netlify.com/portfolio/2015-05-16-synesthesia/</link>
      <pubDate>Sat, 16 May 2015 00:13:37 -0706</pubDate>
      
      <guid>https://sheltron.netlify.com/portfolio/2015-05-16-synesthesia/</guid>
      <description>realtime reactive VJ software
Gravity Current&amp;rsquo;s Synesthesia In June 2015 I worked with Gravity Current on their new Synesthesia software for live reactive visuals. For this project I created two &amp;ldquo;Scenes&amp;rdquo; that played at a couple of shows in Austin:
Looper&amp;rsquo;s night @ Strange Brew Featured my reaction diffusion system ported to C++ OpenGL with a render-to-texture feedback loop for simulation. This was a sweet event not only because of all the talented musicians, but we were able to get MIDI data from the loop pedals into Synesthesia and trigger different layers based on which loops were going.</description>
    </item>
    
  </channel>
</rss>